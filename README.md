# ETL Pipelines for Data Science Workflows

Good-quality data is very important in data science, but it often comes from many different places and in messy formats. Some data comes from databases, while others come from files or websites. This raw data is hard to use right away, and so we need to clean and organize it first.

**ETL** is the process that helps with this. **ETL** stands for **E**xtract, **T**ransform, and **L**oad. 
- Extract means collecting data from different sources. 
- Transform means cleaning and formatting the data. 
- Load means storing the data in a database for easy access.

Building ETL pipelines automates this process. A strong ETL pipeline saves time and makes data reliable.

In this repository are Python notebooks with information on how to build ETL pipelines for data science projects.
Building ETL pipelines helps essentially to automate data science projects.
